# -*- coding: utf-8 -*-
"""Assignment_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GTVCB-rxeXKV12tApHLeHFQWQJSWcZ17
"""

#Import the libraries numpy and pandas
import pandas as pd
import numpy as np

#The dataset is uploaded and converted into a dataframe
df = pd.read_csv('/content/Assignment3.csv')
df.head(10)

# Converting values to float for better training
for col in df.columns:
  df[col] = df[col].astype(float)

df.head()

"""#Prepare the dataset for regression"""

X = np.array(df[['x1','x2','x3','x4','x5']])
Y = np.expand_dims(df['y'],1)

"""#Task-1
Implementing OLS on the data directly and evaluating the baseline SSE loss.
"""

numerator = np.dot(X.T,Y)
denominator = np.dot(X.T,X)
denominator_inv = np.linalg.inv(denominator)
beta = np.dot(denominator_inv, numerator)
print(beta)

y_predict = np.dot(X,beta)

#Defining a helper function
def SSE(y,yhat):
  return np.sum((y-yhat)**2)

print(SSE(Y,y_predict))

"""# Using sklearn's linear_model package to build the OLS model"""

from sklearn import linear_model
model1 = linear_model.LinearRegression()
model1.fit(X,Y)
yhat_base = model1.predict(X)
loss = SSE(Y, yhat_base)
print("Beta :", model1.coef_, "Bias :", model1.intercept_)
print("Loss :", loss)

"""###The loss is observed to be very high.

# Task 2
# Main Objectives:
## 1. Performing EDA to understand predictor features and how are they influencing each other.
## 2. To study how each individual predictor influence the output variable
## 3. Particularly use correlation study to estimate the influence
## 4. To show necessary visualization and its representative interpretations to subtantiate my inferences
## 5. To get the required features and respective transformation

### Let's check the statistics of the features and outputs
"""

df.describe()

df.info()

# Commented out IPython magic to ensure Python compatibility.
# Importing matplotlib and seaborn for EDA
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""#Checking for outliers in target values"""

sns.boxplot(x=df['y'])

"""It seems there are no outliers"""

# Generating a heatmap for correlation study
plt.figure(figsize=(10,10))
sns.heatmap(df.corr(),annot=True)

"""# Check the features that are correlating heavily"""

# To find which features are highly correlated with each other
corr = np.corrcoef(X.T)
print(corr>0.9)

# For rough idea, seeing how significant is the correlation is between features, and between feature and target
all_data = np.concatenate((X,Y), axis=1)
corr_all = np.corrcoef(all_data.T)
print(corr_all)
print(corr_all>0.7)

"""# We use pairplot to explore relationships between multiple variables in a dataset."""

sns.pairplot(df)

"""#Though we have done pairplot, for better understanding, we use scatterplot to understand relationship between each of the features and target values."""

fig, axes = plt.subplots(4, 1, figsize=(10, 12))

# Scatter plot in each subplot
axes[0].scatter(X[:, 0], Y, color='r')  # First subplot
axes[0].set_title("feature x1 vs Target")

axes[1].scatter(X[:, 1], Y, color='b')  # Second subplot
axes[1].set_title("feature x2 vs Target")

axes[2].scatter(X[:, 2], Y, color='g')  # Third subplot
axes[2].set_title("feature x3 vs Target")

axes[3].scatter(X[:, 3], Y, color='m')  # Fourth subplot
axes[3].set_title("feature x4 vs Target")

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

"""#It is observed that x1 and x4 are linearly proportional to target

# Idea for transformation:
1. We can remove either x1 or x4, and make a fit using it.
2. We can't make a direct judgement about the behaviour of x2, but it is also linearly related to target
3. x3 is negatively correlated, as x3 is increasing->target is decreasing
4. We can normalise x1 and x3, and make a fit. Also, we can try to transform x1 to x1^2 since, the coefficient for x1 obtained in OLS is very high.We can also try and see if log(x3) does something in reducing the error.

# Task - 3
1. Fitting the OLS on selected and transformed features
"""

# Apply transformations to x1 and x3
df['x1_squared'] = df['x1']**2
df['x3_log'] = np.log(df['x3'])  # Assuming x3 has positive values

#First fit using transformed features having only x1_squared, x2,x3 and x5

X_transformed_1 = np.array(df[['x1_squared', 'x2','x3', 'x5']])

# Train the model with transformed features
model1 = linear_model.LinearRegression()
model1.fit(X_transformed_1, Y)
yhat = model1.predict(X_transformed_1)
loss = SSE(Y, yhat)
print("Beta :", model1.coef_, "Bias :", model1.intercept_)
print("Loss :", loss)

# Now use the transformed features having only x1_squared, x2, x3_log, x5.
#Only change is we are using x3_log, in place of x_3
X_transformed_2 = np.array(df[['x1_squared', 'x2','x3_log', 'x5']])

# Train the model with transformed features
model2 = linear_model.LinearRegression()
model2.fit(X_transformed_2, Y)
yhat = model2.predict(X_transformed_2)
loss = SSE(Y, yhat)
print("Beta :", model2.coef_, "Bias :", model2.intercept_)
print("Loss :", loss)

"""# There is no significant change in using x3_log in place of x3

#Now I will try to use polynomial features to fit the target values.
"""

# Importing PolynomialFeatures for creating polynomial features
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Assume `df` is your dataframe with features in columns 'X1', 'X2', 'X3', 'X4', 'X5' and target in 'y'
X = df[['x1', 'x2', 'x3', 'x5']]
y = df['y']

# Generate polynomial features of degree 2
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly2 = poly.fit_transform(X)

# Fit the linear regression model
model3 = LinearRegression()
model3.fit(X_poly2, y)

# Get the coefficients of the model
coefficients = model3.coef_

# Get the corresponding terms for each coefficient
terms = poly.get_feature_names_out(X.columns)

# Combine terms and coefficients
coeff_term_mapping = list(zip(terms, coefficients))

y_pred_poly_2 = model3.predict(X_poly2)
# Display the mapping
for term, coeff in coeff_term_mapping:
    print(f"Term: {term}, Coefficient: {coeff}")
loss = SSE(y, y_pred_poly_2)
print("Loss :", loss)

"""#We can take the degree to 3 now. And see if the error decreases, based on which we can make future decisions."""

poly = PolynomialFeatures(degree=3, include_bias=False)
X_poly3 = poly.fit_transform(X)

# Fit the linear regression model
model4 = LinearRegression()
model4.fit(X_poly3, y)

# Get the coefficients of the model
coefficients = model4.coef_

# Get the corresponding terms for each coefficient
terms = poly.get_feature_names_out(X.columns)

# Combine terms and coefficients
coeff_term_mapping = list(zip(terms, coefficients))

y_pred_poly_3 = model4.predict(X_poly3)
# Display the mapping
for term, coeff in coeff_term_mapping:
    print(f"Term: {term}, Coefficient: {coeff}")
loss = SSE(y, y_pred_poly_3)
print("Loss :", loss)

"""#As we increase the degree, the loss will decrease further, but the model will start overfitting the curve.

#Task-4
1. Install the LazyRegressor class
2. Build the LazyRegressor model and compare the RMSE reported by all the regression models of it against our previous OLS losses.
3. Find the reasons for why different techniqes report different performance metrics.
"""

#install lazypredict
!pip install lazypredict

from lazypredict.Supervised import LazyRegressor

# Use the entire dataset for both training and testing to match the OLS comparison
X_train, X_test, y_train, y_test = X, X, y, y

# Initialize and fit LazyRegressor
lazy_regressor = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = lazy_regressor.fit(X_train, X_test, y_train, y_test)

# Display the RMSE values from LazyRegressor
print(models)

# Check the columns in the models DataFrame
print(models.columns)

#Import mean_squared_error metric from sklearn
from sklearn.metrics import mean_squared_error

#Our baseline(naive) OLS model fit RMSE
baseline_rmse = np.sqrt(mean_squared_error(y, yhat_base))
print(f"Baseline OLS RMSE: {baseline_rmse}")

# Calculate RMSE for the transformed OLS model(degree 2)
transformed_rmse = np.sqrt(mean_squared_error(y, y_pred_poly_2))
print(f"Transformed OLS RMSE: {transformed_rmse}")

"""# Few models like GaussianProcessRegressor,               DecisionTreeRegressor,              ExtraTreeRegressor,   ExtraTreesRegressor,XGBRegressor(nearly zero) are having better RMSE than our transformed OLS but remaining models of LazyRegressor are doing a mediocre job.

# The difference in performance metrics across models arise due to the inherent nature of each models, including its complexity, sensitivity to data, assumptions, and the ability to capture the underlying patterns in the data. Simpler models may perform well on clean, linear datasets, while more complex models like ensemble methods excel in capturing intricate relationships but require careful tuning and are prone to overfitting.

# Comparing these models on a single dataset helps understand their strengths and weaknesses, guiding the selection of the most appropriate model for a given problem. The performance of a model is not just about the accuracy or RMSE but also about understanding why a model performs the way it does and how it generalizes to new data.
"""