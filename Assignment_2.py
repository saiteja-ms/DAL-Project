# -*- coding: utf-8 -*-
"""Assignment_2_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-MbpCG8XsF_guSQMG5grZPe8bNSHs2q

#Importing the required libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model

"""#Uploading the Dataset"""

data = pd.read_csv('/content/Assignment2.data',sep='\t')

data.head(10)

"""# Taking out the SpringPos Column values"""

Y = np.array(data.SpringPos)
X = np.expand_dims(np.arange(226),1)

"""#Implementing OLS Estimate"""

numerator = np.matmul(np.transpose(X),Y)
denominator = np.matmul(np.transpose(X),X)
denom_inv = np.linalg.inv(denominator)
m_ols = np.matmul(denom_inv, numerator)
y_ols = m_ols*X

#Calculating the sum of squared error
def SSE(y,x,m):
  return np.sum((y-np.matmul(x,m))**2)

print(f'Minimum SSE using OLS is {SSE(Y,X,m_ols)}')
print(f'Slope that minimizes SSE using OLS: {m_ols[0]}')

#Implementing a linear search for theta
SSE_linear_search = []
theta_values = np.arange(0,100,5)
for theta in theta_values:
  m_linear_search = np.tan(np.pi*theta/180)
  SSE_linear_search.append(SSE(Y,X,np.expand_dims(m_linear_search,0)))

#Getting the optimal Theta(Theta can be assumed to be a parameter that maps X to Y)
optimal_theta_idx = np.argmin(SSE_linear_search)
optimal_theta = theta_values[optimal_theta_idx]

print(f'theta that minimizes SSE: {optimal_theta} degrees')
print(f'Minimum SSE using linear search is {SSE_linear_search[optimal_theta_idx]}')

y_linsearh = np.tan(np.pi*optimal_theta/180)*X
plt.plot(theta_values, SSE_linear_search, c='b', marker='x')
plt.xlabel('Theta (in degrees)')
plt.ylabel('SSE')
plt.title('SSE vs Theta (Linear Search)')
plt.show()

"""#Implementing SKlearn's package for Linear Regression"""

Model = linear_model.LinearRegression(fit_intercept = False)
Model.fit(X,Y)

m_sklearn = Model.coef_[0]
print(f'Slope that minimizes SSE using sklearn: {m_sklearn}')
print(f'Minimum SSE using sklearn is {SSE(Y,X,np.expand_dims(m_sklearn,0))}')

plt.scatter(X,Y, c='r', marker='+')
plt.plot(X,y_ols,c='b', label='OLS')
plt.plot(X,y_linsearh,c='g', label='Linear Search')
plt.plot(X,Model.predict(X),c='y', label='sklearn')
plt.title('Spring Position with Linear Fits vs Time')
plt.xlabel('Time (s)')
plt.ylabel('Spring Position')
plt.legend()
plt.show()

"""## Comparing OLS and Linear Search Models

I'm using the Ordinary Least Squares (OLS) closed-form solution, and the Linear Search model is exactly the same. However, it differs from the solution we obtained using `sklearn`.

The Linear Search fit isn't ideal, as we're not searching over all possible values of \( m \) that minimize the Sum of Squared Errors (SSE). If we decrease the step size from 5, the Linear Search solution might converge to the OLS solution.

### Analysis of Fit

As we can see from the plot, neither OLS nor Linear Search provides a good fit for the data. This suggests the need for feature transformation using domain knowledge.

### Feature Transformation using Domain Knowledge

From the visualized plot of the data, we observe that the **Time period** (or frequency) remains constant while the **Amplitude** decreases, a phenomenon known as **oscillatory decay**.

The analytical solution of a spring-mass damped system suggests that the position is directly proportional to the product of:

- $(e^{-\gamma t})$ and $( \sin\omega t)$ (where ${\gamma} > 0$ controls the decay rate of the amplitude).

### New Feature and Model

We use the proportionality term as our new feature and estimate the proportionality constant using `sklearn` linear regression.

#Implementing Sinusoidal plot in a Linear manner

Based on observation, we can see that there are 3.5 cycles within the time with 226 datapoints(assumed)
i.e., 3.5xTime_period = 225
      Angular_frequency = 2* pi/T
"""

Omega = 2*np.pi/(2*225/7)

"""The equation which we are trying to approximate based on our knowledge from spring systems:

$y(t) =  m \cdot e^{-\gamma t} \cdot \sin(\omega t)$

Where, our model tries to predict $m$.


"""

#Defining a function which takes a given data and returns decay parameter corresponding to the minimum SSE
def decay_parameter(X,Y):
# Create features for linear regression
  gamma = np.arange(0,1,0.005)
  SSE_decay = []
  #Performing some transformation
  for i in gamma:
    X_ = np.exp(-i * X) * np.sin(Omega * X)

    # Linear regression to find m1 and m2
    model = linear_model.LinearRegression()
    model.fit(X_, Y)
    SSE_decay.append(np.sum((Y - model.predict(X_))**2))
  min_idx = np.argmin(SSE_decay)
  plt.plot(gamma,SSE_decay)
  plt.xlabel('Decay parameter')
  plt.ylabel('SSE for corresponding parameter')
  plt.title('Variation of SSE with gamma')
  plt.show()
  return gamma[min_idx], SSE_decay[min_idx]

# Taking whole data as training data
gamma_min, SSE_min = decay_parameter(X,Y)
X_new = np.exp(-gamma_min * X) * np.sin(Omega * X) # Transformed features
model2 = linear_model.LinearRegression()
model2.fit(X_new,Y)
y_decay = model2.predict(X_new)
print(f"SSE after using feature transformation is {np.round(SSE_min,2)}")

#Plotting the predicted curve
plt.scatter(X,Y, c='r', marker='+')
plt.plot(X,y_decay,c='b', label='Decay')
plt.plot(X,Model.predict(X),c='y', label='sklearn')
plt.title('Spring Position with Linear Fits(Including Sinusoidal) vs Time')
plt.xlabel('Time (s)')
plt.ylabel('Spring Position')
plt.legend()

"""#Implementing Interpolation Task"""

from sklearn.model_selection import train_test_split
#Split the dataset into train, val and test using a seed
X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5,random_state=42)

gamma_interpol, SSE_interpol = decay_parameter(X_train,Y_train)
#Transforming the features
x_trans_interpol = np.exp(-gamma_interpol*X)*np.sin(Omega*X)
x_trans_train_interp = np.exp(-gamma_interpol*X_train)*np.sin(Omega*X_train)
x_trans_val_interp = np.exp(-gamma_interpol*X_val)*np.sin(Omega*X_val)
x_trans_test_interp = np.exp(-gamma_interpol*X_test)*np.sin(Omega*X_test)

#Fit linear model for the training data
model_3 = linear_model.LinearRegression()
model_3.fit(x_trans_train_interp,Y_train)

# Predicting y using linear regression for test, val and whole data
y_interpol = model_3.predict(x_trans_interpol)
y_val_interpol = model_3.predict(x_trans_val_interp)
y_test_interpol = model_3.predict(x_trans_test_interp)

# Calculate SSE for each split and total dataset
SSE_interpol_train = np.sum((Y_train-model_3.predict(x_trans_train_interp))**2)
SSE_interpol_val = np.sum((Y_val-y_val_interpol)**2)
SSE_interpol_test = np.sum((Y_test-y_test_interpol)**2)
SSE_interpol = np.sum((Y-y_interpol)**2)

#Printing the metrics obtained
print(f"SSE after using feature transformation is {np.round(SSE_interpol,2)}")
print(f"SSE after using feature transformation on Training set is {np.round(SSE_interpol_train,2)} ")
print(f"SSE after using feature transformation on validation set is {np.round(SSE_interpol_val,2)}")
print(f"SSE after using feature transformation on test set is {np.round(SSE_interpol_test,2)}")

#Plot the predicted curve
plt.scatter(X_train,Y_train,c='b',label = 'Trained Data')
plt.scatter(X_val,Y_val,c='r',label='eval data')
plt.scatter(X_test,Y_test,c='orange',label = 'Test data')
plt.plot(X,y_interpol,c='y',linestyle = '--',label = 'interpolation prediction')
plt.xlabel('Time')
plt.ylabel('Spring position')
plt.title(f'Interpolation, Achieved SSE : {np.round(SSE_interpol,2)}')
plt.legend()
plt.show()

"""# Implementing the Extrapolation task"""

#EXTRAPOLAION
n = len(X)
train_size = int(0.7 * n)
val_size = int(0.15 * n)

#For extrapolation, val and test sets should be outside of the training range .. so split the data as follows
x_train_extrapol = X[:train_size]
y_train_extrapol = Y[:train_size]
x_val_extrapol = X[train_size:train_size + val_size]
y_val_extrapol = Y[train_size:train_size + val_size]
x_test_extrapol = X[train_size + val_size:]
y_test_extrapol = Y[train_size + val_size:]

gamma_extrapol,SSE_extrapol = decay_parameter(x_train_extrapol,y_train_extrapol) #retreive gamma which minimises SSE

#Transform features
x_trans_extrapol = np.exp(-gamma_extrapol*X)*np.sin(Omega*X)
x_trans_train_extrapol = np.exp(-gamma_extrapol*x_train_extrapol)*np.sin(Omega*x_train_extrapol)
x_trans_val_extrapol = np.exp(-gamma_extrapol*x_val_extrapol)*np.sin(Omega*x_val_extrapol)
x_trans_test_extrapol = np.exp(-gamma_extrapol*x_test_extrapol)*np.sin(Omega*x_test_extrapol)

md4 = linear_model.LinearRegression()
md4.fit(x_train_extrapol,y_train_extrapol) # fit linear regression between labels and transformed features

#predicted y values  for val, test and total data
y_extrapol_train = md4.predict(x_train_extrapol)
y_extrapol = md4.predict(X)
y_extrapol_val_predict = md4.predict(x_val_extrapol)
y_extrapol_test_predict = md4.predict(x_test_extrapol)

#Calculate SSE for each split and for whole dataset
SSE_extrapol_train = np.round(np.sum((y_train_extrapol-y_extrapol_train)**2),2)
SSE_extrapol_val = np.round(np.sum((y_val_extrapol-y_extrapol_val_predict)**2),2)
SSE_extrapol_test = np.round(np.sum((y_test_extrapol-y_extrapol_test_predict)**2),2)
SSE_extrapol_overall = np.round(np.sum((Y-y_extrapol)**2),2)

#print metrics
print("Extrapolation")
print(f"Min SSE for the training data is {np.round(SSE_extrapol,2)} at gamma = {gamma_extrapol}")
print(f"SSE for the train data is {SSE_extrapol_train}")
print(f"SSE for the val data is {SSE_extrapol_val}")
print(f"SSE for the test data is {SSE_extrapol_test}")
print(f"SSE for the overall data is {SSE_extrapol_overall}")

#plot the predicted curve
plt.scatter(x_train_extrapol,y_train_extrapol,c='b',label = 'Trained Data')
plt.scatter(x_val_extrapol,y_val_extrapol,c='r',label='eval data')
plt.scatter(x_test_extrapol,y_test_extrapol,c='orange',label = 'Test data')
plt.plot(X,y_extrapol,linestyle = '--',label = 'extrapolation prediction')
plt.xlabel('Time')
plt.ylabel('Spring position')
plt.title(f'Extrapolation, Achieved SSE : {SSE_extrapol_overall}')
plt.legend()
plt.show()

"""# Based on sinusoidal prediction, training on complete dataset and interpolation task gives the best result among all the possibilities compared to training based on extrapolation. This might be due to the need to capturing the generalized features from the entire dataset rather than sticking particularly to one region of dataset and predicting later on val and test set based on extrapolation."""