# -*- coding: utf-8 -*-
"""Assignment_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tmOX2X1CS-Glsklus2qZfuQwLzuZ16xV

# TASK 1
## 1. To measure the label distribution (prior probability) of the predictions made by "DummyBinaryClassifier".
## 2. To implement three random generators(Normal, Bernoulli & Uniform) distributions.
## 3. To draw a line plot with p as x-axis and Pr(True) as y-axis.
"""

# Import the essential libraries
import numpy as np
# Importing BaseEstimator from sklearn to create a custom classifier by extending this base class
from sklearn.base import BaseEstimator
# Importing the Bernoulli distribution from scipy.stats to generate random samples from the Bernoulli distribution
from scipy.stats import bernoulli
# Importing Counter from the collections module to count the occurrences of elements in an iterable
from collections import Counter

"""#Consider a SporadicClassifier that returns a random label in {True, False} for any test input that's fed to it. This classifier does not require any training! Hope, that was already obvious to you. Implement this "DummyBinaryClassifier" as a Python class by extending the BaseEstimator class of sklearn, so that you have mandatory methods such as fit(X, y) and predict(X) are implemented. As your guess, the fit()method would be a dummy 'pass', but the predict() method would return True or False randomly."""

# Implement the DummyBinaryClassifier class
class DummyBinaryClassifier(BaseEstimator):
  # Constructor to initialize the classifier with parameters p (probability) and method (distribution type)
  def __init__(self, p=0.5, method='uniform_random'):
    # Ensure p is between 0 and 1; default to 0.5 if out of bounds
    self.p = 0.5 if p < 0.0 or p > 1.0 else p
    # Select method from the allowed options; default to 'uniform_random'
    self.method = method if method in ["uniform_random", "bernoulli", "normal"] else "uniform_random"

  # Dummy fit method since this classifier doesn't need to train
  def fit(self, X, y=None):
    pass

  # Predict method to generate random binary outputs based on the selection method
  def predict(self, X):
    # we center the normal distribution at 0.5 instead of 0.0
    if self.method == "normal":
        return (0.5 + np.random.randn(X.shape[0])) < self.p
    # Bernoulli distribution with probability p
    elif self.method == "bernoulli":
        return np.bool_(bernoulli.rvs(self.p, size=X.shape[0]))
    # Uniform random distribution compared to probability p (default method)
    else:
        return np.random.rand(X.shape[0]) < self.p

"""# It does not matter what the dataset is, as the classifier is not depending on the inputs."""

# Let's create a dataset of size 1000 instances.
X = np.random.rand(1000)

"""# Let's compute the label distribution for different configuration of the classifier"""

# Instantiate the SporadicClassifier with p=0.3, and the 'normal' method
cla = DummyBinaryClassifier(p=0.3, method='normal')
# Generate predictions for the input data X using the predict method
y = cla.predict(X)
# Use Counter to count the occurrences of each label (True/False) in the predictions
c = Counter(y)

#Create a dictionary comprehension to calculate the proportion of True/False values
# The result is a dictionary where the keys are the labels (True/False),
# and the values are the proportions of those labels in the predictions.
{i[0]: i[1] / len(y) for i in c.items()}

"""# Create reusable functions"""

def compute_prior(y):
  # initialize the counter object on the 'y' labels
  c = Counter(y)
  # Convert the labels into class proportions
  props = {i[0]:i[1]/len(y) for i in c.items()}
  if True not in props:
    props[True] = 0.0
  if False not in props:
    props[False] = 0.0
  return props

"""# Extract the probability of True predictions for the dataset using different random generators."""

p_vals = np.arange(0., 1., 0.1)
b_vals = []
g_vals = []
u_vals = []
for p in p_vals:
  # Spawn the DummyBinaryClassifier with bernoulli random sample generator
  cla = DummyBinaryClassifier(p=p, method='bernoulli')
  # Predict the labels for the input
  y = cla.predict(X)
  # Compute priors
  props = compute_prior(y)
  # Pick the probability of True class
  b_vals.append(props[True])

  # Spawn the DummyBinaryClassifier with gaussian random sample generator
  y = DummyBinaryClassifier(p=p, method='normal').predict(X)
  g_vals.append(compute_prior(y)[True])

  # Spawn the DummyBinaryClassifier with uniform random sample generator
  y = DummyBinaryClassifier(p=p, method='uniform_random').predict(X)
  u_vals.append(compute_prior(y)[True])

"""# Plot the trends side on the same plot for comparison"""

# Use matplotlib's pyplot class
import matplotlib.pyplot as plt
# Plotting True probability values of  Bernoulli, Gaussian and Uniform-random distribution vs p values
plt.plot(p_vals, b_vals)
plt.plot(p_vals, g_vals)
plt.plot(p_vals, u_vals)
plt.xlabel('p')
plt.ylabel('Pr(True)')
plt.title('Label proportion at different values of $p\in[0,1]$')
plt.legend(['Bernoulli', 'Normal','Uniform'], loc='upper left')
plt.show()

"""# If the number of samples are increased, the plots tend to understandably become a straight line

# Task-2
## 1. To take the IRIS dataset, and convert it into a binary class dataset by choosing the majority class as class True and the remaining two classes as class False.

## 2. To use bernoulli version of the DummyBinaryClassifier, we have to predict the following:
   (i) Label prior of the binary IRIS dataset.
   (ii) Precision, Recall, F1 of the predictions at different choice of p-values in [0,1] in steps of 0.1 & also to plot P,R,C as line plots.
   (iii) AUPRC and AURoC
## 3. To plot PRC
## 4. To plot RoC using TPR and FPR
"""

# Importing the Iris dataset from sklearn
from sklearn.datasets import load_iris

'''Importing metrics for evaluating classifier performance: precision,recall,
F1 score, ROC curve and AUC'''
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve

# Load the iris dataset into a dictionary-like object with features and target labels
iris = load_iris()
# Assign the feature matrix(input data) to X and target labels to y
X, y = iris.data, iris.target

# Convert the dataset to binary classification: class 0 (majority) as True, others as False
y_binary = np.where(y == 0, 1, 0)

"""# Reporting the Label Prior"""

# Compute label prior
prior = compute_prior(y_binary)
print(f"Label prior (True class): {prior[True]:.2f}, (False class): {prior[False]:.2f}")

# Generating an array of p values
p_values = np.arange(0,1.1, 0.1)
precision_list, recall_list,f1_list = [], [] ,[]
predictions = []

"""# Computing the precision, recall and F1 score for the corresponding p values"""

# Capture the precision, recall and f1 score for each of the p values
for p in p_values:
  classifier = DummyBinaryClassifier(method='bernoulli', p=p)
  classifier.fit(X, y_binary)
  y_pred = classifier.predict(X)
  predictions.append(y_pred)

  precision = precision_score(y_binary, y_pred)
  recall = recall_score(y_binary, y_pred)
  f1 = f1_score(y_binary, y_pred)

  precision_list.append(precision)
  recall_list.append(recall)
  f1_list.append(f1)

  print(f"p: {p:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}")

"""# Plotting the P,R,C as line plot"""

# Plot Precision, Recall, F1 Score at different p-values
plt.figure(figsize=(10, 6))
plt.plot(p_values, precision_list, label='Precision', marker='*')
plt.plot(p_values, recall_list, label='Recall', marker='*')
plt.plot(p_values, f1_list, label='F1 Score', marker='*')
plt.xlabel('p')
plt.ylabel('Score')
plt.title('Precision, Recall, F1 Score vs p (Bernoulli)')
plt.legend()
plt.show()

"""#"""

# Plot PRC(Precision-Recall curve) using P & R values
for i, p in enumerate(p_vals):
  precision, recall, _ = precision_recall_curve(y_binary, predictions[i])
  plt.figure(figsize=(10, 6))
  # Precision-Recall Curve (PRC)
  plt.plot(recall, precision, label='PRC')
  plt.xlabel('Recall')
  plt.ylabel('Precision')
  plt.title(f'Precision-Recall Curve for p:{p:.2f}')
  plt.legend()
  plt.show()

"""# Plotting RoC curve using TPR and FPR and also computing AURoc ( Area under RoC curve) for each corresponding p-value.

## Short Note: A ROC (which stands for “receiver operating characteristic”) curve is a graph that shows a classification model performance at all classification thresholds. It is a probability curve that plots two parameters, the True Positive Rate (TPR) against the False Positive Rate (FPR), at different threshold values and separates a so-called ‘signal’ from the ‘noise.’
"""

# Plot ROC curve using TPR and FPR
for i, p in enumerate(p_vals):
  fpr, tpr, _ = roc_curve(y_binary, predictions[i])
  auroc = auc(fpr, tpr)
  plt.figure(figsize=(10, 6))
  # ROC Curve
  plt.plot(fpr, tpr)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.show()
  print(f"Area Under ROC Curve (AURoC) for p:{p:.2f}: {auroc:.2f}")

"""# Now, reporting AUPRC and AURoC values
## AUPRC means Area under PRC curve
"""

# Report AUPRC and AURoC
for i, p in enumerate(p_vals):
  precision, recall, _ = precision_recall_curve(y_binary, predictions[i])
  auprc = auc(recall, precision)
  print(f"Area Under Precision-Recall Curve (AUPRC) for p value {p:.2f}: {auprc:.2f}")

"""# Task 3
# Visualization of Decision Boundaries
## In this task, we'll visualize the decision boundaries induced by "DummyBinaryClassifier" at different values of "p" using the three random generators.
"""

# Import ListedColormap to create custom color maps for plots, particularly for coloring decision boundaries
# It allows specifying discrete colors for categories or classes in visualizations.
from matplotlib.colors import ListedColormap

# Define the function to visualize decision boundaries
def plot_decision_boundary(classifier, X, y, p,method, axes):
    # Generating the x-axis and y-axis limits using the first two features of the dataset
    # This defines the range for the grid used to plot the decision boundary
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

    # Create a meshgrid over the feature space for plotting the decision boundary
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))

    # Predict class labels for each point in the meshgrid
    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])
    # Reshape the predictions to match the shape of the meshgrid for plotting
    Z = Z.reshape(xx.shape)

    # Create color maps: light colors for the decision boundary regions and bold colors for the points
    # Light gray and light cyan for decision boundary regions
    cmap_light = ListedColormap(['#D3D3D3', '#E0FFFF'])
    # Bold black and bold cyan for actual data points
    cmap_bold = ListedColormap(['#000000', '#00FFFF'])

    # Plot the decision boundary with light colors
    axes.contourf(xx, yy, Z, cmap=cmap_light)

    # Scatter plot the data points using bold colors
    axes.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)
    # Set the title for the plot, including the probability p and method used in the classifier
    axes.set_title(f'Decision Boundary (p={p}, method={method})')

"""# Generating the visualization of the decision boundaries induced by the DummyBinaryClassifier at different values of p for all the three random generators."""

# Test different values of p and random generators to visualize the decision boundary
fig, axes = plt.subplots(3,5, figsize=(30,18))
p_values = [0, 0.25, 0.5, 0.75, 1] # Define different p values to test

# Implementing the generation of decision boundary
for j, method in enumerate(['bernoulli', 'normal', 'uniform_random']):
  for i, p in enumerate(p_values):
      classifier = DummyBinaryClassifier(method=method, p=p)
      classifier.fit(X, y_binary)
      plot_decision_boundary(classifier, X, y_binary,p, method, axes[j][i])

plt.tight_layout()
plt.show()

